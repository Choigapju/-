# -*- coding: utf-8 -*-
"""gemma-2b-it.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cX4uwJrSNikGFiwfniVRszvPkB8K7vxo
"""

!huggingface-cli login

import torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import re

model_name = "google/gemma-1.1-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_advice(user_input):
    prompt = f"""
    역할: 학습 코치
    태스크: 주어진 학습 시간 분배를 분석하고, 균형 잡힌 학습을 위한 조언을 제공해 주세요.

    사용자: {user_input}

    코치: 학습 시간 분배 분석:
    """
    input_ids = tokenizer.encode(prompt, return_tensors="pt")

    output = model.generate(input_ids, max_length=500, num_beams=5, num_return_sequences=1, temperature=0.7, top_k=50, top_p=0.95)

    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    ai_response = generated_text.split("코치:")[-1].strip()

    def postprocess_response(response):
        response = response.replace("AI:", "").replace("사용자:", "").strip()
        response = response.capitalize()
        return response

    def filter_response(response):
        response = re.sub(r'(.+?)\1+', r'\1', response)
        if "코치:" in response:
            response = response.split("코치:")[-1].strip()
        return response

    ai_response = postprocess_response(ai_response)
    ai_response = filter_response(ai_response)

    return ai_response

while True:
    user_input = input("사용자: ")
    if user_input.lower() == 'quit':
        break

    advice = generate_advice(user_input)
    print(f"코치: {advice}\n")

